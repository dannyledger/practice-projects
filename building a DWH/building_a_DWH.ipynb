{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30b0f44",
   "metadata": {},
   "source": [
    "_Learning to build a simple data warehouse gives me practical insight into how data flows from raw sources to actionable insights. Even though full-scale architecture is the responsibility of data architects, understanding the basics of modelling, ETL, and schema design helps me collaborate better with engineers, solve data issues more effectively, and approach analytics with a true end-to-end perspective._\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338f517",
   "metadata": {},
   "source": [
    "### **Step 1** | Scoping out the project\n",
    "\n",
    "---\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Develop a modern data warehouse using SQL Server to consolidate sales data, enabling analytical reporting and informed decision-making.\n",
    "\n",
    "#### Specifications\n",
    "\n",
    "- **Data Sources**: Import data from two source systems (ERP and CRM) provided as CSV files.\n",
    "- **Data Quality**: Cleanse and resolve data quality issues prior to analysis.\n",
    "- **Integration**: Combine both sources into a single, user-friendly data model designed for analytical queries.\n",
    "- **Scope**: Focus on the latest dataset only; historization of data is not required.\n",
    "- **Documentation**: Provide clear documentation of the data model to support both business stakeholders and analytics teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85f446",
   "metadata": {},
   "source": [
    "#### Medallion Architecture\n",
    "\n",
    "As per the course’s recommendation, we will be using the Medallion data management paradigm. This layered approach is particularly effective for building a modern data warehouse because it separates raw, cleansed, and curated datasets into clear stages—commonly referred to as Bronze, Silver, and Gold layers. By structuring the pipeline this way, we ensure that each layer has a distinct purpose: Bronze holds unaltered, raw source data; Silver refines and cleanses this data for reliability; and Gold aggregates it into business-ready tables optimised for reporting and analytics.\n",
    "\n",
    "Opting for Medallion provides several advantages. It improves data quality and trust by ensuring transformations are traceable and reproducible, as all data first lands in a raw state before being standardised and validated. It also enhances maintainability and scalability, allowing us to debug issues at the appropriate layer rather than across the entire warehouse. \n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 1 image](assets/img/journal_fig1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "| | Bronze Layer | Silver Layer | Gold Layer |\n",
    "| - | - | - | - |\n",
    "| **Definition** | Raw, unprocessed data as-is from sources | Clean and standardised data | Business-ready data | \n",
    "| **Objective** | Traceability & debugging | (Intermediate layer) Prepare data for analysis | Provide data to be consumed for reporting & analytics |\n",
    "| **Object Type** | Tables | Tables | Views |\n",
    "| **Load Method** | Full load (truncate & insert) | Full load (truncate & insert) | None |\n",
    "| **Data Transformation** | None (as-is) | Data cleaning, standardisation, normalisation, enrichment & derived columns | Data integration, aggregation, business logic & rules |\n",
    "| **Data Modeling** | None (as-is) | None (as-is) | Start schema, aggregated objects, flat tables |\n",
    "| **Target Audience** | Data engineers | Data engineers & analysts | Data analysts & business users |  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4775880",
   "metadata": {},
   "source": [
    "#### Layers for Separation of Concerns (SoC)\n",
    "\n",
    "The above layers mean that we have separation of concerns (SoC) - an important principle where we take a complex system and break it down into independent parts, each focused on a specific responsibility or operation without overlapping with others. So for a data warehouse, SoC means breaking the architecture into independent layers—such as ingestion, transformation, storage, and consumption—so each layer handles its own responsibility without interfering with the others...\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 2 image](assets/img/journal_fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1573535",
   "metadata": {},
   "source": [
    "#### Beginnings...\n",
    "\n",
    "Moving forward, this journal will document each stage of the data warehouse build with clear, structured notes. We will capture:\n",
    "\n",
    "1. **Data Source Overview** – Key details about each source, including its format, refresh frequency, volume, and method of access.\n",
    "2. **Data Quality and Validation** – How we identify and resolve issues such as missing data, duplicates, and inconsistencies, as well as checks between Bronze and Silver layers.\n",
    "3. **Medallion Layer Objectives** – The purpose and responsibilities of the Bronze, Silver, and Gold layers, along with how each supports data trust and reporting readiness.\n",
    "4. **Target Schema Design** – Sketches and notes on fact and dimension tables, including any decisions around star vs. snowflake schema design.\n",
    "5. **ETL / ELT Flow** – Steps for moving data between layers, whether through incremental or full loads, and how the process will be orchestrated.\n",
    "6. **Testing and Monitoring** – Plans for data validation, quality checks, and ongoing monitoring to ensure the reliability of the Gold outputs.\n",
    "7. **Versioning and Progress Log** – A running journal of decisions, challenges, lessons learned, and key milestones as the project evolves.\n",
    "\n",
    "#### The high-level goal\n",
    "\n",
    "![data architecture figure 3 image](assets/img/journal_fig3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf77ee",
   "metadata": {},
   "source": [
    "### **Step 2** | Establishing rules & naming conventions\n",
    "\n",
    "---\n",
    "\n",
    "#### Naming Conventions\n",
    "\n",
    "We need naming conventions to keep our data systems clear, consistent, and scalable. They let us understand datasets and pipelines at a glance, reduce errors, and make automation with tools like Airflow or dbt much easier. Consistent names help us collaborate better, onboard new team members faster, and simplify governance tasks like lineage and documentation. With predictable patterns in place, our Medallion or multi-source warehouse can grow without becoming messy or confusing.\n",
    "\n",
    "#### General Principles\n",
    "- **Naming Conventions**: Use snake_case, with lowercase letters and underscores (_) to separate words.\n",
    "- **Language**: Use English for all names.\n",
    "- **Avoid Reserved Words**: Do not use SQL reserved words as object names.\n",
    "\n",
    "#### Table Naming Conventions\n",
    "\n",
    "**Bronze Layer Rules**\n",
    "\n",
    "All names must start with the source system name, and table names must match their original names without renaming.\n",
    "- `[sourcesystem_entity]`\n",
    "    - `[sourcesystem]`: Name of the source system (e.g., crm, erp).\n",
    "    - `[entity]`: Exact table name from the source system.\n",
    "    - Example: `crm_customer_info` → Customer information from the CRM system.\n",
    "\n",
    "**Silver Layer Rules**\n",
    "\n",
    "In this scenario, we are not renaming items between Bronze and Silver. So the rules above will apply in Silver. \n",
    "\n",
    "**Gold Layer Rules**\n",
    "\n",
    "All names must use meaningful, business-aligned names for tables, starting with the category prefix.\n",
    "- `[category_entity]`\n",
    "    - `[category]`: Describes the role of the table, such as dim (dimension) or fact (fact table).\n",
    "    - `[entity]`: Descriptive name of the table, aligned with the business domain (e.g., customers, products, sales).\n",
    "    - Examples:\n",
    "        - `dim_customers` → Dimension table for customer data.\n",
    "        - `fact_sales` → Fact table containing sales transactions.\n",
    "\n",
    "**Glossary of Category Patterns**\n",
    "| Pattern | Meaning | Example(s) |\n",
    "| - | - | - |\n",
    "| `dim_` | Dimension table | `dim_customer`, `dim_product` | \n",
    "| `fact_` | Fact table | `fact_sales` | \n",
    "| `agg_` | Aggregated table | `agg_customer`, `agg_sales_monthly` | \n",
    "\n",
    "\n",
    "#### Column Naming Conventions\n",
    "\n",
    "**Surrogate Keys**\n",
    "\n",
    "- All primary keys in dimension tables must use the suffix `_key`.\n",
    "- `[table_name]_key`\n",
    "    - [table_name]: Refers to the name of the table or entity the key belongs to.\n",
    "    - `_key`: A suffix indicating that this column is a surrogate key.\n",
    "    - Example: `customer_key` → Surrogate key in the dim_customers table.\n",
    "\n",
    "**Technical Columns**\n",
    "\n",
    "- All technical columns must start with the prefix dwh_, followed by a descriptive name indicating the column's purpose.\n",
    "- `dwh_[column_name]`\n",
    "    - `dwh`: Prefix exclusively for system-generated metadata.\n",
    "    - `[column_name]`: Descriptive name indicating the column's purpose.\n",
    "    - Example: `dwh_load_date` → System-generated column used to store the date when the record was loaded.\n",
    "\n",
    "**Stored Procedure**\n",
    "\n",
    "All stored procedures used for loading data must follow the naming pattern: \n",
    "- `load_[layer]`.\n",
    "    - `[layer]`: Represents the layer being loaded, such as bronze, silver, or gold.\n",
    "    - Example:\n",
    "        - `load_bronze` → Stored procedure for loading data into the Bronze layer.\n",
    "        - `load_silver` → Stored procedure for loading data into the Silver layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fe172",
   "metadata": {},
   "source": [
    "### **Step 3** | Ready the database & schema\n",
    "\n",
    "---\n",
    "\n",
    "To begin capturing our design into code - we need to create and select a database, followed by establishing the schemas (which reflect our layers).\n",
    "\n",
    "A key rule to note with any shared scripts - particularly high-level ones (see below), is to include a clear description for each, as well as warnings regarding its purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7b8fe",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "=============================================================\n",
    "Create Database and Schemas\n",
    "=============================================================\n",
    "Script Purpose:\n",
    "    This script creates a new database named 'DataWarehouse' after checking if it already exists. \n",
    "    If the database exists, it is dropped and recreated. Additionally, the script sets up three schemas \n",
    "    within the database: 'bronze', 'silver', and 'gold'.\n",
    "\t\n",
    "W A R N I N G :\n",
    "    Running this script will drop the entire 'DataWarehouse' database if it exists. \n",
    "    All data in the database will be permanently deleted. Proceed with caution \n",
    "    and ensure you have proper backups before running this script.\n",
    "*/\n",
    "\n",
    "USE master;\n",
    "GO\n",
    "\n",
    "-- Drop and recreate the 'DataWarehouse' database\n",
    "IF EXISTS (SELECT 1 FROM sys.databases WHERE name = 'DataWarehouse')\n",
    "BEGIN\n",
    "    ALTER DATABASE DataWarehouse SET SINGLE_USER WITH ROLLBACK IMMEDIATE;\n",
    "    DROP DATABASE DataWarehouse;\n",
    "END;\n",
    "GO\n",
    "\n",
    "-- create the DWH database\n",
    "CREATE DATABASE DataWarehouse;\n",
    "GO\n",
    "\n",
    "USE DataWarehouse;\n",
    "GO\n",
    "\n",
    "-- create schemas \n",
    "CREATE SCHEMA bronze;\n",
    "GO\n",
    "\n",
    "CREATE SCHEMA silver;\n",
    "GO\n",
    "\n",
    "CREATE SCHEMA gold;\n",
    "GO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd114b",
   "metadata": {},
   "source": [
    "### **Step 4** | Developing the Bronze Layer\n",
    "\n",
    "---\n",
    "\n",
    "For us to begin building the first layer of our warehouse's architecture, we like any discipline, need to carefully **understand the sources** and the context surrounding them. We need to meet with stakeholders, and gauge:\n",
    "\n",
    "**Business Context & Ownership**\n",
    "- Who owns the data?\n",
    "- What business process it supports?\n",
    "- Systems and data documentation\n",
    "- Data model and data catalog \n",
    "\n",
    "**Architecture & Technology Stack**\n",
    "- How is data stored? \n",
    "- What are the integration capabilities?\n",
    "\n",
    "**Extract & Load**\n",
    "- Incremental vs. full load?\n",
    "- Data scope & historical needs\n",
    "- What is the expected size of the extracts?\n",
    "- Are there any data volume limitations?\n",
    "- How to avoid impacting the source system's performance?\n",
    "- Authentication and authorisation \n",
    "\n",
    "#### Specification of the Bronze Layer\n",
    "\n",
    "The Bronze Layer will handle raw, un-processed data as-is from sources. The overall objective of this layer is traceability and debugging.\n",
    "\n",
    "We are doing a full-load (truncate and insert), producing tables.\n",
    "\n",
    "To get started, we will explore the data to identify the column names and data types (ie. **Data Profiling**)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "376fffbe",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "cst_id,cst_key,cst_firstname,cst_lastname,cst_marital_status,cst_gndr,cst_create_date\n",
    "11000,AW00011000, Jon,Yang ,M,M,2025-10-06\n",
    "11001,AW00011001,Eugene,Huang  ,S,M,2025-10-06\n",
    "11002,AW00011002,Ruben, Torres,M,M,2025-10-06\n",
    "11003,AW00011003,Christy,  Zhu,S,F,2025-10-06\n",
    "11004,AW00011004, Elizabeth,Johnson,S,F,2025-10-06\n",
    "11005,AW00011005,Julio,Ruiz,S,M,2025-10-06\n",
    "11006,AW00011006,Janet,Alvarez,S,F,2025-10-06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610beb5",
   "metadata": {},
   "source": [
    "_previewing 8 of 15000+ lines_\n",
    "\n",
    "Referencing the data source above, and the naming conventions we set for each layer, we create our DDL (Data Definition Language). We repeat this process for each table to a total of six. These will store the raw data from the two sources folders (each holding three CSV files respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6686f46",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- if this table exists, remove it and create a new one\n",
    "IF OBJECT_ID ('bronze.crm_prd_info', 'U')  IS NOT NULL\n",
    "    DROP TABLE bronze.crm_prd_info;\n",
    "-- table creation with the required columns\n",
    "CREATE TABLE bronze.crm_prd_info (\n",
    "    prd_id INT,\n",
    "    prd_key NVARCHAR(50),\n",
    "    prd_nm NVARCHAR(50),\n",
    "    prd_cost INT,\n",
    "    prd_line NVARCHAR(2),\n",
    "    prd_start_dt DATE,\n",
    "    prd_end_dt DATE\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23eb0a",
   "metadata": {},
   "source": [
    "Once complete, we have six tables in the Bronze layer schema, of which all clearly reference their source system. \n",
    "\n",
    "```\n",
    "└── Bronze Layer/                        # schema           \n",
    "    ├── bronze.crm_cust_info/            # table for CRM \n",
    "    ├── bronze.crm_prd_info/\n",
    "    ├── bronze.crm_sales_details/        \n",
    "    ├── bronze.erp_cust_az12/            # table for ERP   \n",
    "    ├── bronze.erp_loc_a101/        \n",
    "    └── bronze.erp_px_cat_gtv2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadac893",
   "metadata": {},
   "source": [
    "Following the structure setup, we then load the data via a bulk, truncate and insert process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6f9a6",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Prep the table for first load by making sure it is empty, or if previously loaded, avoid a duplication error\n",
    "TRUNCATE TABLE bronze.crm_cust_info;\n",
    "\n",
    "-- load the data from file\n",
    "BULK INSERT bronze.crm_cust_info\n",
    "FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\cust_info.csv'\n",
    "-- below is we where provide the specification for the upload\n",
    "WITH (\n",
    "    FIRSTROW = 2, -- skip header row\n",
    "    FIELDTERMINATOR = ',', -- specify the field delimiter\n",
    "    TABLOCK\n",
    ");\n",
    "\n",
    "-- check results\n",
    "SELECT * FROM bronze.crm_cust_info;\n",
    "\n",
    "-- check row count exc. the first row\n",
    "SELECT COUNT(*) FROM bronze.crm_cust_info;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120f277",
   "metadata": {},
   "source": [
    "Once we have basic structure in place, we can repeat for each data source, adding extensive formatting, keywords including `PRINT`, `TRY`, `DECLARE` throughout the code as a stored procedure. Doing such provides users with critical feedback, particularly in both system status and maintainance/debugging. This heavily increases the lines of code, but as Baraa Salkini notes:\n",
    "> _'...this is what data engineering is all about, not just loading the data, but engineering the entire pipeline, measuring the speed, accounting for errors, to print and capture each step in the ETL process.'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2135035",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR ALTER PROCEDURE dbo.load_bronze AS \n",
    "BEGIN\n",
    "    DECLARE @start_time DATETIME, @end_time DATETIME, @batch_start_time DATETIME, @batch_end_time DATETIME;\n",
    "    BEGIN TRY\n",
    "        SET @batch_start_time = GETDATE();\n",
    "        PRINT '====================================';\n",
    "        PRINT 'Loading data into bronze layer...';\n",
    "        PRINT '====================================';\n",
    "        PRINT '------------------------------------';\n",
    "        PRINT 'Loading CRM tables ...';\n",
    "        PRINT '------------------------------------';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.crm_cust_info';\n",
    "        -- Prep the table for first load by making sure it is empty, or if previously loaded, avoid a duplication error\n",
    "        TRUNCATE TABLE bronze.crm_cust_info;\n",
    "        -- load the data from file\n",
    "        BULK INSERT bronze.crm_cust_info\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\cust_info.csv'\n",
    "        -- below is we where provide the specification for the upload\n",
    "        WITH (\n",
    "            FIRSTROW = 2, -- skip header row\n",
    "            FIELDTERMINATOR = ',', -- specify the field delimiter\n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.crm_prd_info';\n",
    "        TRUNCATE TABLE bronze.crm_prd_info;\n",
    "        BULK INSERT bronze.crm_prd_info\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\prd_info.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2, \n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.sales_details';\n",
    "        TRUNCATE TABLE bronze.crm_sales_details;\n",
    "        BULK INSERT bronze.crm_sales_details\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\sales_details.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2,\n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "\n",
    "        PRINT '------------------------------------';\n",
    "        PRINT 'Loading ERP tables ...';\n",
    "        PRINT '------------------------------------';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.cust_az12';\n",
    "        TRUNCATE TABLE bronze.erp_cust_az12;\n",
    "        BULK INSERT bronze.erp_cust_az12\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_erp\\cust_az12.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2, \n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.loc_101';\n",
    "        TRUNCATE TABLE bronze.erp_loc_a101;\n",
    "        BULK INSERT bronze.erp_loc_a101\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_erp\\loc_a101.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2, \n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: px_cat_g1v2';\n",
    "        TRUNCATE TABLE bronze.erp_px_cat_g1v2;\n",
    "        BULK INSERT bronze.erp_px_cat_g1v2\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_erp\\px_cat_g1v2.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2,\n",
    "            FIELDTERMINATOR = ',',\n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        \n",
    "        SET @batch_end_time = GETDATE();\n",
    "        PRINT '====================================';\n",
    "        PRINT 'Data loaded into bronze layer successfully.';\n",
    "        PRINT 'Total Load Duration: ' + CAST(DATEDIFF(second, @batch_start_time, @batch_end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        PRINT '====================================';\n",
    "    END TRY\n",
    "    BEGIN CATCH\n",
    "        PRINT '====================================';\n",
    "        PRINT 'Error occurred while loading data into Bronze layer:';\n",
    "        PRINT 'Error Message ' + ERROR_MESSAGE();\n",
    "        PRINT 'Error Message ' + CAST(ERROR_NUMBER() AS NVARCHAR(10));     \n",
    "        PRINT 'Error Message ' + CAST(ERROR_STATE() AS NVARCHAR(10));  \n",
    "        PRINT '====================================';\n",
    "    END CATCH\n",
    "END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4611be",
   "metadata": {},
   "source": [
    "So that `EXEC dbo.load_bronze;` returns..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279d3fb",
   "metadata": {},
   "source": [
    "```\n",
    "Started executing query at Line 1\n",
    "====================================\n",
    "Loading data into bronze layer...\n",
    "====================================\n",
    "------------------------------------\n",
    "Loading CRM tables ...\n",
    "------------------------------------\n",
    ">> Truncating table: bronze.crm_cust_info\n",
    "(18493 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: bronze.crm_prd_info\n",
    "(397 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: bronze.sales_details\n",
    "(60398 rows affected)\n",
    ">> Load Duration: 1 seconds\n",
    "------------------------------------\n",
    "Loading ERP tables ...\n",
    "------------------------------------\n",
    ">> Truncating table: bronze.cust_az12\n",
    "(18483 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: bronze.loc_101\n",
    "(18484 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: px_cat_g1v2\n",
    "(37 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    "====================================\n",
    "Data loaded into bronze layer successfully.\n",
    "Total Load Duration: 1 seconds\n",
    "====================================\n",
    "Total execution time: 00:00:00.294\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e1290",
   "metadata": {},
   "source": [
    "### **Step 5** | Developing the Silver Layer\n",
    "\n",
    "---\n",
    "\n",
    "#### Capturing the Data Flow\n",
    "\n",
    "With our source data _entering the warehouse_, we need to keep track of where it is going. We create a data flow diagram to capture where the data comes from, where it ends up, and how it moves through each layer. While similar to a high-level architecture diagram, it focuses solely on the data itself. This helps us develop the data lineage—a critical concept for maintaining reliable data pipelines.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 4 image](assets/img/journal_fig4.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70983266",
   "metadata": {},
   "source": [
    "#### Workflow for Silver Layer\n",
    "\n",
    "Similar to Bronze, where we needed to explore the context and requirements. With Silver we need to explore and understand the data itself, principally because we are transitioning from raw data, to clean, standardiised data. In order to achieve this, we follow these steps.\n",
    "\n",
    "1. Review the data in its Bronze layer state.\n",
    "2. Data cleaning\n",
    "    - Check data quality\n",
    "    - Write data transformations\n",
    "    - Insert into silver layer\n",
    "3. Perform data correctness checks\n",
    "    - If errors or concerns appear -> cycle back to previous steps\n",
    "4. Data documentation\n",
    "\n",
    "#### Reviewing the data & anticipating an integration model\n",
    "\n",
    "To quickly preview a table in _SQL Server_, we can right click on table in the connections directory and 'SELECT' each, inserting it into a new query (executing one by one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f270986",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
